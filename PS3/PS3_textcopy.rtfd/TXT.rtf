{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;\f1\fswiss\fcharset0 Helvetica;\f2\fmodern\fcharset0 Courier;
\f3\fmodern\fcharset0 Courier-Oblique;\f4\fmodern\fcharset0 Courier-Bold;\f5\fnil\fcharset0 AppleSymbols;
}
{\colortbl;\red255\green255\blue255;\red41\green101\blue168;\red36\green43\blue141;\red245\green245\blue245;
\red51\green110\blue109;\red16\green121\blue2;\red169\green14\blue26;\red15\green112\blue1;\red204\green46\blue18;
\red151\green0\blue255;\red0\green0\blue255;\red118\green0\blue2;\red11\green84\blue1;\red56\green110\blue165;
\red0\green0\blue120;\red183\green144\blue6;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid101\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720

\f0\b\fs52 \cf0 \expnd0\expndtw0\kerning0
Regression analysis in distributed computing\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
First thing to note is that with big amounts of data closed form solution through normal equations is not feasible anymore:\
\pard\pardeftab720\qc

\f1\b\fs34 \cf0 \expnd0\expndtw0\kerning0
w
\b0 \expnd0\expndtw0\kerning0
=(
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
T
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1\i \cf0 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
\uc0\u8722 1
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
T
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1\i \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
Computing
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
T
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
term takes
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
O
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
n
\f0\i0 \expnd0\expndtw0\kerning0
\

\f1\i \expnd0\expndtw0\kerning0
k
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
2
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
operations, while inverse takes
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
O
\i0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\

\f1\i \expnd0\expndtw0\kerning0
k
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
3
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
, where
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
n
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
is number of observations and
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
k
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
is number of variables. Storage is also considerable,
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
O
\i0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\

\f1\i \expnd0\expndtw0\kerning0
k
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
2
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
for these matrices and
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
O
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
nk
\i0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
for matrix
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
.\
How can we deal with these issues? Depends on the type of the problem we have.\
\pard\tx220\tx720\pardeftab720\li720\fi-720
\ls1\ilvl0
\b \cf0 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Few variables, but many observations
\b0 \expnd0\expndtw0\kerning0
. Storing 
\f1\i\fs34 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\fs28 \expnd0\expndtw0\kerning0
can be solved by distributed form of storage, while computing 
\f1\i\fs34 \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\f1\i\fs24 \expnd0\expndtw0\kerning0
T
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\uc0\u8232 
\f1\i \expnd0\expndtw0\kerning0
X
\f0\i0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\fs28 \expnd0\expndtw0\kerning0
matrix can be done through a sum of outer products instead of computing inner products. Outer products work great as we can do them separately on each node and then sum them together in the main node. This is feasible if number of dimensions is not that big, in such case matrices of size 
\f1\i\fs34 \expnd0\expndtw0\kerning0
k
\f0\i0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\f1\fs24 \expnd0\expndtw0\kerning0
2
\f0\fs34 \expnd0\expndtw0\kerning0
\uc0\u8232 \u8232 
\fs28 \expnd0\expndtw0\kerning0
should be able to fit on a single node.\
\ls1\ilvl0
\b \kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Many variables, and many observations
\b0 \expnd0\expndtw0\kerning0
. Outer product operation is now also not feasible as matrices of size 
\f1\i\fs34 \expnd0\expndtw0\kerning0
k
\f0\i0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\f1\fs24 \expnd0\expndtw0\kerning0
2
\f0\fs34 \expnd0\expndtw0\kerning0
\uc0\u8232 \u8232 
\fs28 \expnd0\expndtw0\kerning0
do not fit in any single node. In these situations we can potentially exploit sparsity to reduce the storage and computational requirements. We can also try to reduce the dimensionality through unsupervised learning techniques, such as SVD or PCA. However, the way out is really to discard the closed form approach and use numerical optimization - (stochastic) gradient descent needs less operations, 
\f1\i\fs34 \expnd0\expndtw0\kerning0
O
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
nk
\i0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\fs28 \expnd0\expndtw0\kerning0
, and uses far less storage, 
\f1\i\fs34 \expnd0\expndtw0\kerning0
O
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
k
\i0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\uc0\u8232 
\fs28 \expnd0\expndtw0\kerning0
.\
\pard\pardeftab720

\b\fs52 \cf0 \expnd0\expndtw0\kerning0
Logistic regression\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
Now we will examine how we can use a chain map-reduce operations to train the logistic regression with gradient descent. We will use the Bank marketing dataset from the {\field{\*\fldinst{HYPERLINK "https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 UCI repository}} to illustrate these operations. It is not a big dataset but it will serve to illustrate the principles. We could have worked with bigger datasets, but it would have involved a lot of waiting for some of the steps.\
Outline:\
\pard\tx220\tx720\pardeftab720\li720\fi-720
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Create an RDD out of the text file\
\ls2\ilvl0\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Parse text lines into variables\
\ls2\ilvl0\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
Normalize variables\
\ls2\ilvl0\kerning1\expnd0\expndtw0 {\listtext	4	}\expnd0\expndtw0\kerning0
Create training, validation and test set\
\ls2\ilvl0\kerning1\expnd0\expndtw0 {\listtext	5	}\expnd0\expndtw0\kerning0
Baseline model\
\ls2\ilvl0\kerning1\expnd0\expndtw0 {\listtext	6	}\expnd0\expndtw0\kerning0
Logistic regression through gradient descent\
\ls2\ilvl0\kerning1\expnd0\expndtw0 {\listtext	7	}\expnd0\expndtw0\kerning0
Logistic regression through the MLlib Spark library\
\pard\pardeftab720

\b\fs44 \cf0 \expnd0\expndtw0\kerning0
1. Creating an RDD and first look at the data\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
I provided you with the dataset in a csv format, use that one instead of original dataset from the repository. I only removed the categorical variables. These need special processing operations that we will not cover. You should load the file directly from your bucket and convert it to the RDD using the appropriate function. You should have created a bucket already on S3 and uploaded the dataset there.\
Following the instructions in the comments, fill in the parts in the code below instead of (YOUR_CODE) parts.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[16]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# number of workers you have started
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
numPartitions = \cf6 \expnd0\expndtw0\kerning0
3\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# we are loading the text file directly from our bucket and converting it to RDD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# use appropriate function and set the number of partitions
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
rdd = sc.textFile(\cf7 \expnd0\expndtw0\kerning0
"s3://pysparkhomework/data/bank.csv"\cf0 \expnd0\expndtw0\kerning0
)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf8 \cb4 \expnd0\expndtw0\kerning0
type\cf0 \expnd0\expndtw0\kerning0
(rdd)\cb1 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720
\cf9 \expnd0\expndtw0\kerning0
Out[16]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
pyspark.rdd.RDD\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[17]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# use appropriate function to count the number of observations in the dataset
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
noObs = rdd.count()\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noObs\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# use appropriate function to get only 3 observations from the whole dataset
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
noObservations = \cf6 \expnd0\expndtw0\kerning0
3\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
fewObs = rdd.takeSample(\cf8 \expnd0\expndtw0\kerning0
False\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
3\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 fewObs\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
41188\
[u'0,30,19,4.858,5191,0,1.1,93.994,-36.4', u'0,37,26,4.967,5228.1,0,1.4,94.465,-41.8', u'0,51,130,0.982,4963.6,2,-1.1,94.601,-49.5']\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[18]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# checking if you got a correct result, you should have 11 million observations
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# you will see an output here only if you made an error
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noObs 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf6 \expnd0\expndtw0\kerning0
41188\cf0 \expnd0\expndtw0\kerning0
, \cf7 \expnd0\expndtw0\kerning0
"Something is wrong here!"\cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
len\cf0 \expnd0\expndtw0\kerning0
(fewObs) 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noObservations, \cf7 \expnd0\expndtw0\kerning0
"You did not extract correct number of observations"\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs44 \cf0 \expnd0\expndtw0\kerning0
2. Parsing the text lines\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
From the print of 
\f2 \expnd0\expndtw0\kerning0
fewObs
\f0 \expnd0\expndtw0\kerning0
 object you should realize that each observation in the current RDD is one long line of text, and not nice a data frame.\
One line should look something like this:\
\pard\pardeftab720

\f2 \cf0 \expnd0\expndtw0\kerning0
u'1,8.692932128906250000e-01,-6.350818276405334473e-01,2.256902605295181274e-01,...'
\f0 \expnd0\expndtw0\kerning0
.\
First point is a label, either a 0 or 1, and remaining numbers are features.\
Your first task is to parse the text lines and create variables out of them. We will convert them to a special class 
\f2 \expnd0\expndtw0\kerning0
LabeledPoint
\f0 \expnd0\expndtw0\kerning0
 from MLlib library (see official docs {\field{\*\fldinst{HYPERLINK "http://spark.apache.org/docs/latest/mllib-data-types.html"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 here}}). This is the format used for storing supervised learning data in MLlib, both classification and regression.\
You will fill out the details of the 
\f2 \expnd0\expndtw0\kerning0
parseTextLine
\f0 \expnd0\expndtw0\kerning0
 function that processes one line of text and returns a LabeledPoint object. You will check it out on couple of observations that you extracted above, on 
\f2 \expnd0\expndtw0\kerning0
fewObs
\f0 \expnd0\expndtw0\kerning0
 object.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[19]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# numpy version should be at least 1.4 for MLlib, there is a ridiculous bug that raises an exception
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# because recent numpy is named 1.10 which is mathematically not higher than 1.4
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# the bootstrap script installs previous version, 1.9.2 for this reason
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 numpy 
\f4\b \cf8 \expnd0\expndtw0\kerning0
as
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np.__version__\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
1.10.4\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[20]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
from
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 pyspark.mllib.regression 
\f4\b \cf8 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 LabeledPoint\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 numpy 
\f4\b \cf8 \expnd0\expndtw0\kerning0
as
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# you should fill out details of function parseTextLine
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: line, a single observation consisting of a string
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: LabeledPoint instance, text converted to distinct variables, label and features
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# use Python's split() method to separate elements of the text lines
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
parseTextLine\cf0 \expnd0\expndtw0\kerning0
(line):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    obs = LabeledPoint(line.split(\cf7 \expnd0\expndtw0\kerning0
","\cf0 \expnd0\expndtw0\kerning0
)[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
], line.split(\cf7 \expnd0\expndtw0\kerning0
","\cf0 \expnd0\expndtw0\kerning0
)[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
:])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 obs\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# check if it works on fewObs object by applying parseTextLine(), the result should be a list
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
parsedLines = [parseTextLine(line) 
\f4\b \cf8 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 line 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 fewObs]\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
type\cf0 \expnd0\expndtw0\kerning0
(parsedLines)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# now, print out the features and label separately for the first observation, 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# using the LabeledPoint.features and LabeledPoint.label attributes
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 parsedLines[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
].features\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 parsedLines[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
].label\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
<type 'list'>\
[37.0,26.0,4.967,5228.1,0.0,1.4,94.465,-41.8]\
0.0\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[21]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
len\cf0 \expnd0\expndtw0\kerning0
(parsedLines[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
].features) 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf6 \expnd0\expndtw0\kerning0
8\cf0 \expnd0\expndtw0\kerning0
, \cf7 \expnd0\expndtw0\kerning0
'You should have 8 features!'\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
If everything seems to be in order, proceed by parsing all the data.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[22]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# use your parseTextLine function on every observation
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
parsedRDD = rdd.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(parseTextLine)\cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs44 \cf0 \expnd0\expndtw0\kerning0
3. Normalizing the features\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
Standardizing or normalizing features is very relevant for algorithms using distance measures. In our situation it will also bring significant improvements since we will use (stochastic) gradient descent to train the logistic regression. We will use a simple transformation to 0-1 interval\
\pard\pardeftab720

\f1\i\fs34 \cf0 \expnd0\expndtw0\kerning0
x
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
norm
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
=(
\i \expnd0\expndtw0\kerning0
x
\i0 \expnd0\expndtw0\kerning0
\uc0\u8722 
\i \expnd0\expndtw0\kerning0
min
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
x
\i0 \expnd0\expndtw0\kerning0
))/(
\i \expnd0\expndtw0\kerning0
max
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
x
\i0 \expnd0\expndtw0\kerning0
)\uc0\u8722 
\i \expnd0\expndtw0\kerning0
min
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
x
\i0 \expnd0\expndtw0\kerning0
))
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f2\fs28 \cf3 \expnd0\expndtw0\kerning0
In\'a0[109]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# take a single row and compute the number of features in it
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
noFeatures = \cf8 \expnd0\expndtw0\kerning0
len\cf0 \expnd0\expndtw0\kerning0
(parsedRDD.collect()[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
].features)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noFeatures  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# you should get 8
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
8\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[12]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# get min and max for each feature 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
featuresMin = []; featuresMax = []\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 i 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
range\cf0 \expnd0\expndtw0\kerning0
(noFeatures):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 i\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    feature = parsedRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: lp.features[i])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    featuresMin.append(feature.\cf8 \expnd0\expndtw0\kerning0
min\cf0 \expnd0\expndtw0\kerning0
())\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    featuresMax.append(feature.\cf8 \expnd0\expndtw0\kerning0
max\cf0 \expnd0\expndtw0\kerning0
())\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# print out minimum and maximum value of each feature
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 featuresMin\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 featuresMax\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0\
1\
2\
3\
4\
5\
6\
7\
[17.0, 0.0, 0.63400000000000001, 4963.6000000000004, 0.0, -3.3999999999999999, 92.200999999999993, -50.799999999999997]\
[98.0, 4918.0, 5.0449999999999999, 5228.1000000000004, 7.0, 1.3999999999999999, 94.766999999999996, -26.899999999999999]\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[110]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# working it out
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
lp = parsedRDD.collect()[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
]\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
lp\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#diff = lp.features - featuresMin
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#minmaxdiff = [x - y for x,y in zip(featuresMax, featuresMin)]
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#norm = [a / b for a,b in zip(diff, minmaxdiff)]
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#normlp = LabeledPoint(lp.label, norm)
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#normlp
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#lp.features - 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720
\cf9 \expnd0\expndtw0\kerning0
Out[110]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
LabeledPoint(0.0, [57.0,149.0,4.857,5191.0,0.0,1.1,93.994,-36.4])\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[29]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# define a function for normalizing each feature according to min and max information
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: lp, LabeledPoint, a single observation
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#        featuresMin, a list of minimum values for each fature
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#        featureMax, a list of maximum values for each feature
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: a LabeledPoint with original label, but new, transformed features
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
normalize\cf0 \expnd0\expndtw0\kerning0
(lp, featuresMin=featuresMin, featuresMax=featuresMax):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    diff = lp.features 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 featuresMin\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    minmaxdiff = [x 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 y 
\f4\b \cf8 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x,y 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
zip\cf0 \expnd0\expndtw0\kerning0
(featuresMax, featuresMin)]\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    norm = [a 
\f4\b \cf10 \expnd0\expndtw0\kerning0
/
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 b 
\f4\b \cf8 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 a,b 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
zip\cf0 \expnd0\expndtw0\kerning0
(diff, minmaxdiff)]\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    normlp = LabeledPoint(lp.label, norm)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 normlp\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# run normalize() on all observations
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
normedRDD = parsedRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(normalize)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# if everything went all right, you should see 0 and 1 as a result here
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 normedRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: lp.features[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
]).\cf8 \expnd0\expndtw0\kerning0
min\cf0 \expnd0\expndtw0\kerning0
()\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 normedRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: lp.features[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
]).\cf8 \expnd0\expndtw0\kerning0
max\cf0 \expnd0\expndtw0\kerning0
()\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.0\
1.0\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs44 \cf0 \expnd0\expndtw0\kerning0
4. Creating training, validation and test data\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
Now that we have created proper dataset we will create separate training, validation and test data. You should use the {\field{\*\fldinst{HYPERLINK "https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 randomSplit method}} to randomly create three separate datasets, use the weights and seed supplied below.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[30]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# do not change the weights and the seed
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
weights = [\cf6 \expnd0\expndtw0\kerning0
.7\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
.15\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
.15\cf0 \expnd0\expndtw0\kerning0
]\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
seed = \cf6 \expnd0\expndtw0\kerning0
1111\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# use randomSplit with weights and seed defined above on the trainRDD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
trainRDD, valRDD, testRDD = normedRDD.randomSplit(weights, seed)\cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
Now you should {\field{\*\fldinst{HYPERLINK "https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 cache}} all three of new RDD's, as you will be using them a lot in the remainder of the problem set.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[31]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# cache the data
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
trainRDD.cache()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
valRDD.cache()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
testRDD.cache()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# get the number of observation in each subset
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
noTrain = trainRDD.count()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
noVal = valRDD.count()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
noTest = testRDD.count()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
"Training: \{0:.3f\}, Validation: \{1:.3f\}, Test: \{2:.3f\}, All: \{3:.3f\}"\cf0 \expnd0\expndtw0\kerning0
.\cf8 \expnd0\expndtw0\kerning0
format\cf0 \expnd0\expndtw0\kerning0
(noTrain, noVal, noTest, noTrain 
\f4\b \cf10 \expnd0\expndtw0\kerning0
+
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noVal 
\f4\b \cf10 \expnd0\expndtw0\kerning0
+
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noTest)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
Training: 28850.000, Validation: 6167.000, Test: 6171.000, All: 41188.000\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[32]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noTrain 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf6 \expnd0\expndtw0\kerning0
28769\cf0 \expnd0\expndtw0\kerning0
, \cf7 \expnd0\expndtw0\kerning0
"Wrong number of observations. Did you change the weights?"\cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noVal 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf6 \expnd0\expndtw0\kerning0
6195\cf0 \expnd0\expndtw0\kerning0
, \cf7 \expnd0\expndtw0\kerning0
"Wrong number of observations. Did you change the weights?"\cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 noTest 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf6 \expnd0\expndtw0\kerning0
6224\cf0 \expnd0\expndtw0\kerning0
, \cf7 \expnd0\expndtw0\kerning0
"Wrong number of observations. Did you change the weights?"\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf12 \expnd0\expndtw0\kerning0
---------------------------------------------------------------------------\cf0 \expnd0\expndtw0\kerning0
\
\cf12 \expnd0\expndtw0\kerning0
AssertionError\cf0 \expnd0\expndtw0\kerning0
                            Traceback (most recent call last)\
\pard\pardeftab720
\cf13 \expnd0\expndtw0\kerning0
<ipython-input-32-c126dd90f4b7>\cf0 \expnd0\expndtw0\kerning0
 in \cf14 \expnd0\expndtw0\kerning0
<module>\cf15 \expnd0\expndtw0\kerning0
()\cf14 \expnd0\expndtw0\kerning0
\
\cf13 \expnd0\expndtw0\kerning0
----> 1\cf16 \expnd0\expndtw0\kerning0
 \cf13 \expnd0\expndtw0\kerning0
assert\cf16 \expnd0\expndtw0\kerning0
 noTrain == \cf14 \expnd0\expndtw0\kerning0
28769\cf16 \expnd0\expndtw0\kerning0
, \cf15 \expnd0\expndtw0\kerning0
"Wrong number of observations. Did you change the weights?"\cf16 \expnd0\expndtw0\kerning0
\
\cf13 \expnd0\expndtw0\kerning0
      2\cf16 \expnd0\expndtw0\kerning0
 \cf13 \expnd0\expndtw0\kerning0
assert\cf16 \expnd0\expndtw0\kerning0
 noVal == \cf14 \expnd0\expndtw0\kerning0
6195\cf16 \expnd0\expndtw0\kerning0
, \cf15 \expnd0\expndtw0\kerning0
"Wrong number of observations. Did you change the weights?"\cf16 \expnd0\expndtw0\kerning0
\
\cf13 \expnd0\expndtw0\kerning0
      3\cf16 \expnd0\expndtw0\kerning0
 \cf13 \expnd0\expndtw0\kerning0
assert\cf16 \expnd0\expndtw0\kerning0
 noTest == \cf14 \expnd0\expndtw0\kerning0
6224\cf16 \expnd0\expndtw0\kerning0
, \cf15 \expnd0\expndtw0\kerning0
"Wrong number of observations. Did you change the weights?"\cf16 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf12 \expnd0\expndtw0\kerning0
AssertionError\cf16 \expnd0\expndtw0\kerning0
: Wrong number of observations. Did you change the weights?\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs44 \cf0 \expnd0\expndtw0\kerning0
4. Baseline model\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
For every analysis you should have a good benchmark or baseline as evaluating performance of models is difficult without a reference frame. How do you know whether an accuracy of 75% is a good performance or a bad performance?\
A simplest reference frame is how well could you do with no learning what so ever, that is, without extracting any information from features. In classification problem that we have here, this is a simple proportion of labels in the dataset. In prediction terms, based on this model we will always make the same prediction independent of the features - our average label is the constant prediction value. If our model is doing worse than this, we are doing something very wrong.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[33]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# compute mean over label part of LabeledPoints
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
meanLabel = trainRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: lp.label).mean()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 meanLabel\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.112824956672\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[34]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np.allclose(meanLabel, \cf6 \expnd0\expndtw0\kerning0
0.112586464597\cf0 \expnd0\expndtw0\kerning0
), \cf7 \expnd0\expndtw0\kerning0
"Something is wrong, check your calculation of the mean label"\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf12 \expnd0\expndtw0\kerning0
---------------------------------------------------------------------------\cf0 \expnd0\expndtw0\kerning0
\
\cf12 \expnd0\expndtw0\kerning0
AssertionError\cf0 \expnd0\expndtw0\kerning0
                            Traceback (most recent call last)\
\pard\pardeftab720
\cf13 \expnd0\expndtw0\kerning0
<ipython-input-34-66d97c9fa950>\cf0 \expnd0\expndtw0\kerning0
 in \cf14 \expnd0\expndtw0\kerning0
<module>\cf15 \expnd0\expndtw0\kerning0
()\cf14 \expnd0\expndtw0\kerning0
\
\cf13 \expnd0\expndtw0\kerning0
----> 1\cf16 \expnd0\expndtw0\kerning0
 \cf13 \expnd0\expndtw0\kerning0
assert\cf16 \expnd0\expndtw0\kerning0
 np.allclose(meanLabel, \cf14 \expnd0\expndtw0\kerning0
0.112586464597\cf16 \expnd0\expndtw0\kerning0
), \cf15 \expnd0\expndtw0\kerning0
"Something is wrong, check your calculation of the mean label"\cf16 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf12 \expnd0\expndtw0\kerning0
AssertionError\cf16 \expnd0\expndtw0\kerning0
: Something is wrong, check your calculation of the mean label\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
You should have obtained lower proportion of successful calls than 0.5, hence the prediction of the baseline model for each observation is 0.\
How does the baseline model performs? We will use a misclassification error as a measure of performance. You will now write a function that evaluates the accuracy of each prediction.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[43]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# classify function should take two arguments
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# inputs: probability, float, that the observation is 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#         threshold, float, for classifying predicted probability as 0 or 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: float, predicted label, either 0 or 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
classify\cf0 \expnd0\expndtw0\kerning0
(probability, threshold):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# you can ignore the ties
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
if
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 probability 
\f4\b \cf10 \expnd0\expndtw0\kerning0
>
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 threshold:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        predLabel = \cf6 \expnd0\expndtw0\kerning0
1\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
else
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        predLabel = \cf6 \expnd0\expndtw0\kerning0
0\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 predLabel\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# this function should compute the misclassification error for a single observation and its prediction,
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# you should use classify function to convert probability into predicted labels
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: probability, float between 0 and 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#        predictedLabel, float, either 0 or 1, 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#        threshold, float, for classifying predicted probability as 0 or 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: float, 0 if correct and 1 if incorrect
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
misErrorSingle\cf0 \expnd0\expndtw0\kerning0
(probability, label, threshold):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    predLabel = classify(probability, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
if
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 predLabel 
\f4\b \cf10 \expnd0\expndtw0\kerning0
==
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 label:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        output = \cf6 \expnd0\expndtw0\kerning0
0\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
else
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        output = \cf6 \expnd0\expndtw0\kerning0
1\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 output\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# this function should compute mean misclassification error on RDD labPred (see example below)
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# it should apply the function misErrorSingle on the whole RDD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: labPredRDD, a label prediction tuples
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#        threshold, float, for classifying predicted probability as 0 or 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: mean misclassification error
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
misError\cf0 \expnd0\expndtw0\kerning0
(labPredRDD, threshold):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    output = labPredRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: misErrorSingle(x[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
], x[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
], threshold))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 output.mean()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# check it on an easy example, 1 observation misclassified, 2 correct
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
labPredRDD_ex = sc.parallelize([(\cf6 \expnd0\expndtw0\kerning0
0.\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1.\cf0 \expnd0\expndtw0\kerning0
), (\cf6 \expnd0\expndtw0\kerning0
0.\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
0.\cf0 \expnd0\expndtw0\kerning0
), (\cf6 \expnd0\expndtw0\kerning0
1.\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1.\cf0 \expnd0\expndtw0\kerning0
)])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
threshold = \cf6 \expnd0\expndtw0\kerning0
0.5\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misError_ex = misError(labPredRDD_ex, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 misError_ex\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.333333333333\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[44]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np.allclose(misError_ex, \cf6 \expnd0\expndtw0\kerning0
0.333333333333\cf0 \expnd0\expndtw0\kerning0
), \cf7 \expnd0\expndtw0\kerning0
'incorrect value for misError_ex'\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
You should now make predictions based on the baseline model for all three datasets.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[71]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# set the threshold for evaluating the probabiliteis
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
threshold = \cf6 \expnd0\expndtw0\kerning0
0.5\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# for each dataset first create RDD's of tuples with meanLabel and label in each tuple   
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# and then feed these RDD's into misError function
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
baseTrain = trainRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: (x.label, meanLabel))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
baseTrain_misError = misError(baseTrain, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
baseVal = valRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: (x.label, meanLabel))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
baseVal_misError = misError(baseVal, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
baseTest = testRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: (x.label, meanLabel))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
baseTest_misError = misError(baseTest, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
'Baseline model - Train misclassification error = \{0:.3f\}'\cf0 \expnd0\expndtw0\kerning0
.\cf8 \expnd0\expndtw0\kerning0
format\cf0 \expnd0\expndtw0\kerning0
(baseTrain_misError)\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
'Baseline model - Validation misclassification error = \{0:.3f\}'\cf0 \expnd0\expndtw0\kerning0
.\cf8 \expnd0\expndtw0\kerning0
format\cf0 \expnd0\expndtw0\kerning0
(baseVal_misError)\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
'Baseline model - Test misclassification error = \{0:.3f\}'\cf0 \expnd0\expndtw0\kerning0
.\cf8 \expnd0\expndtw0\kerning0
format\cf0 \expnd0\expndtw0\kerning0
(baseTest_misError)\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
Baseline model - Train misclassification error = 0.113\
Baseline model - Validation misclassification error = 0.106\
Baseline model - Test misclassification error = 0.118\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs44 \cf0 \expnd0\expndtw0\kerning0
5. Training logistic regression with Gradient descent\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
Now let's see how much better we can do with a simple logistic regression. We will train it with the gradient descent method since analytical solutions require too much storage and computing time.\
Recall from the first term the equation for the gradient descent update for logistic regression:\
\pard\pardeftab720

\f1\b\fs34 \cf0 \expnd0\expndtw0\kerning0
w
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\i0 \expnd0\expndtw0\kerning0
+1
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
=
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\b \cf0 \expnd0\expndtw0\kerning0
w
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
\uc0\u8722 
\i \expnd0\expndtw0\kerning0
\uc0\u945 
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
\dn18 \uc0\u8721 
\f0 \expnd0\expndtw0\kerning0
\up0 \
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
n
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\b \cf0 \expnd0\expndtw0\kerning0
w
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f5\fs24 \cf0 \expnd0\expndtw0\kerning0
\uc0\u8868 
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\b \cf0 \expnd0\expndtw0\kerning0
x
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
n
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
\uc0\u8722 
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
n
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\b \cf0 \expnd0\expndtw0\kerning0
x
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
n
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
.
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
where
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
is the iteration number, and
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
n
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
denotes observation. We will use constant learning rate, but there are various schemes for decreasing learning rates, such as
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
\uc0\u945 
\i0 \expnd0\expndtw0\kerning0
=
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
constant
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\

\f1\i\fs24 \expnd0\expndtw0\kerning0
n
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\

\f1\i\fs24 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
\uc0\u8730 
\f0\fs34 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
.\
Although this is computationally and storage wise much cheaper than analytical solution, there are disadvantages as well. Gradient part is computed in each node and sent to the driver (map step), after sum of gradients is computed in the driver (reduce step), updated weights
\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\b \cf0 \expnd0\expndtw0\kerning0
w
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\i0 \expnd0\expndtw0\kerning0
+1
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
have to be redistributed over the network to all the nodes for the next iteration. Hence, there will be a lot of communication over network.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[73]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
from
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 pyspark.mllib.linalg 
\f4\b \cf8 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 DenseVector\cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
You will first define a log loss function. Misclassification error that we defined above is great for getting a sense of absolute performance of the algorithm, however it is too rough to use it for monitoring the gradient descent process. You will first define function 
\f2 \expnd0\expndtw0\kerning0
logLossSingle
\f0 \expnd0\expndtw0\kerning0
 that computes the loss for a single observation, and then function 
\f2 \expnd0\expndtw0\kerning0
logLoss
\f0 \expnd0\expndtw0\kerning0
 that will implement it in parallel on whole RDD.\
Given independent observations likelihood function for binary classification problems can be defined as\
\pard\pardeftab720\qc

\f1\i\fs34 \cf0 \expnd0\expndtw0\kerning0
L
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
\uc0\u952 
\i0 \expnd0\expndtw0\kerning0
)=
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
\dn16 \uc0\u8719 
\f0 \expnd0\expndtw0\kerning0
\up0 \
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\i0 \expnd0\expndtw0\kerning0
=1
\f0\fs34 \expnd0\expndtw0\kerning0
\

\f1\i\fs24 \expnd0\expndtw0\kerning0
N
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1\i \cf0 \expnd0\expndtw0\kerning0
p
\i0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
|
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
x
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1\i \cf0 \expnd0\expndtw0\kerning0
\uc0\u952 
\i0 \expnd0\expndtw0\kerning0
)=
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
\dn16 \uc0\u8719 
\f0 \expnd0\expndtw0\kerning0
\up0 \
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\i0 \expnd0\expndtw0\kerning0
=1
\f0\fs34 \expnd0\expndtw0\kerning0
\

\f1\i\fs24 \expnd0\expndtw0\kerning0
N
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
p
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs16 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
(1\uc0\u8722 
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
p
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\fs24 \cf0 \expnd0\expndtw0\kerning0
1\uc0\u8722 
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs16 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
Since we would be computing very small numbers we would quickly run into problems with numerical precisions. Hence we actually compute a log of this expression, which is a monotonic transformation and no information is lost\
\pard\pardeftab720\qc

\f1\i\fs34 \cf0 \expnd0\expndtw0\kerning0
logL
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
\uc0\u952 
\i0 \expnd0\expndtw0\kerning0
)=\uc0\u8722 
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
\dn16 \uc0\u8721 
\f0 \expnd0\expndtw0\kerning0
\up0 \
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\i0 \expnd0\expndtw0\kerning0
=1
\f0\fs34 \expnd0\expndtw0\kerning0
\

\f1\i\fs24 \expnd0\expndtw0\kerning0
N
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1\i \cf0 \expnd0\expndtw0\kerning0
log
\i0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
p
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
)+(1\uc0\u8722 
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
y
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
)(1\uc0\u8722 
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i \cf0 \expnd0\expndtw0\kerning0
p
\f0\i0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\i\fs24 \cf0 \expnd0\expndtw0\kerning0
i
\f0\i0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
To convert it into a minimization problem we multiply it additionally with -1.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[103]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
from
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 math 
\f4\b \cf8 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 log\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: prob, float, value between 0 and 1, predicted probability that label=1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#        lab, float, label of the observation, either 0 or 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: float, negative log likelihood of a single observation
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
logLossSingle\cf0 \expnd0\expndtw0\kerning0
(prob, lab):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# if probability is too small/large add/subtract this epsilon value
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# this is because log(0) is not defined
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    epsilon = \cf6 \expnd0\expndtw0\kerning0
10e-12\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
if
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np.isclose(prob, \cf6 \expnd0\expndtw0\kerning0
0.0\cf0 \expnd0\expndtw0\kerning0
):\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        prob = prob 
\f4\b \cf10 \expnd0\expndtw0\kerning0
+
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 epsilon\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
elif
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np.isclose(prob, \cf6 \expnd0\expndtw0\kerning0
1.0\cf0 \expnd0\expndtw0\kerning0
):\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        prob = prob 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 epsilon\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# now return the log loss
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
( lab
\f4\b \cf10 \expnd0\expndtw0\kerning0
*
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
log(prob) 
\f4\b \cf10 \expnd0\expndtw0\kerning0
+
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 (\cf6 \expnd0\expndtw0\kerning0
1
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
lab)
\f4\b \cf10 \expnd0\expndtw0\kerning0
*
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
log(\cf6 \expnd0\expndtw0\kerning0
1
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
prob) )\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# if you get an error on any of these check your computations, especially epsilon
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossSingle(\cf6 \expnd0\expndtw0\kerning0
.5\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)   
\f3\i \cf5 \expnd0\expndtw0\kerning0
# 0.69314718056
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossSingle(\cf6 \expnd0\expndtw0\kerning0
.99\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# 0.0100503358535
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossSingle(\cf6 \expnd0\expndtw0\kerning0
.01\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# 4.60517018599
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossSingle(\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# 25.3284360229
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossSingle(\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# 1.00000008275e-11
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.69314718056\
0.0100503358535\
4.60517018599\
25.3284360229\
1.00000008275e-11\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[104]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# logLoss function should compute the mean log loss on the whole RDD probLab 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: probLab, RDD of probability label tuples
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: float, mean log loss
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
logLoss\cf0 \expnd0\expndtw0\kerning0
(probLab):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# apply the function logLossSingle on the whole RDD and compute the mean
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    output = probLab.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: logLossSingle(x[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
], x[\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
])).mean()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 output\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# check it on an easy example
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabRDD_ex = sc.parallelize([(\cf6 \expnd0\expndtw0\kerning0
0.5\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1.\cf0 \expnd0\expndtw0\kerning0
), (\cf6 \expnd0\expndtw0\kerning0
0.99\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
0.\cf0 \expnd0\expndtw0\kerning0
), (\cf6 \expnd0\expndtw0\kerning0
0.01\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1.\cf0 \expnd0\expndtw0\kerning0
)])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLoss_ex = logLoss(probLabRDD_ex)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLoss_ex\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
3.30116251751\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[105]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
assert
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 np.allclose(logLoss_ex, \cf6 \expnd0\expndtw0\kerning0
3.30116251751\cf0 \expnd0\expndtw0\kerning0
), \cf7 \expnd0\expndtw0\kerning0
'Check your logLoss function'\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
Now define a sigmoid function that will be transforming linear combinations of feature values and weights to probabilities\
\pard\pardeftab720\qc

\f1\i\fs34 \cf0 \expnd0\expndtw0\kerning0
f
\i0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
z
\i0 \expnd0\expndtw0\kerning0
)=
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1 \cf0 \expnd0\expndtw0\kerning0
1
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1 \cf0 \expnd0\expndtw0\kerning0
1+
\i \expnd0\expndtw0\kerning0
exp
\i0 \expnd0\expndtw0\kerning0
(\uc0\u8722 
\i \expnd0\expndtw0\kerning0
z
\i0 \expnd0\expndtw0\kerning0
)
\f0 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720

\f2\fs28 \cf3 \expnd0\expndtw0\kerning0
In\'a0[106]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 math\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# define sigmoid function
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: z, a float, result of a dot product between weights and feature values
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: a float, dot product transformed to 0-1 range
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
sigmoid\cf0 \expnd0\expndtw0\kerning0
(z):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf6 \expnd0\expndtw0\kerning0
1
\f4\b \cf10 \expnd0\expndtw0\kerning0
/
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
( \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
 
\f4\b \cf10 \expnd0\expndtw0\kerning0
+
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 math.exp(
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
z))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 sigmoid(\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# should produce 0.5
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 sigmoid(\cf6 \expnd0\expndtw0\kerning0
100\cf0 \expnd0\expndtw0\kerning0
)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# should produce number close to 1
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 sigmoid(
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
100\cf0 \expnd0\expndtw0\kerning0
)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# should produce number close to 0
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.5\
1.0\
3.72007597602e-44\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
We will break down the gradient descent algorithm into components. We will write a function for the part that is computed on separate nodes in parallel - gradient. Then these gradients are summed in the driver node and weights are updated based on the whole sum.\
Define a function that computes the gradient for a single observation\
\pard\pardeftab720\qc

\f1\fs34 \cf0 \expnd0\expndtw0\kerning0
(
\i \expnd0\expndtw0\kerning0
y
\i0 \expnd0\expndtw0\kerning0
\uc0\u8722 
\i \expnd0\expndtw0\kerning0
f
\i0 \expnd0\expndtw0\kerning0
(
\f0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f1\b \cf0 \expnd0\expndtw0\kerning0
w
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f5\fs24 \cf0 \expnd0\expndtw0\kerning0
\uc0\u8868 
\f0\fs34 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\qc

\f1\b \cf0 \expnd0\expndtw0\kerning0
x
\b0 \expnd0\expndtw0\kerning0
))
\b \expnd0\expndtw0\kerning0
x
\f0\b0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\fs28 \cf0 \expnd0\expndtw0\kerning0
.\
Pay particular care that your gradient computation is correct. Error in gradients is the most frequent reason why something goes wrong with gradient descent.\
You will use the 
\f2 \expnd0\expndtw0\kerning0
DenseVector
\f0 \expnd0\expndtw0\kerning0
 {\field{\*\fldinst{HYPERLINK "http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector.dot"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 dot}} method to compute a cross product between feature vector and weight vector. I provided you with an example on which you can test it.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[107]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# compute the gradient for a single observation
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# inputs: weights, an array of regression coefficients; 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#         lp, a LabeledPoint of a single observation
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: DenseVector, an array of values, same length as weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
gradient\cf0 \expnd0\expndtw0\kerning0
(weights, lp):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    crossprod = weights.dot(lp.features)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 (lp.label 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 sigmoid(crossprod)) 
\f4\b \cf10 \expnd0\expndtw0\kerning0
*
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp.features\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
weights_ex = DenseVector([\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
3\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
4\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
5\cf0 \expnd0\expndtw0\kerning0
])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
lp_ex = LabeledPoint(\cf6 \expnd0\expndtw0\kerning0
1.0\cf0 \expnd0\expndtw0\kerning0
, [\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
gradient_ex = gradient(weights_ex, lp_ex)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 gradient_ex\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# you should see a following vector: 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# [3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07]
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# sigmoid should produce smth close to 1 so when it is subtracted from 1 we get smth close to 0
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
[3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07,3.05902226994e-07]\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
Now you should define a function that creates (probability, label) tuples that we will feed into logLoss() function to keep track of the loss function while we are training the algorithm via gradient descent. Keeping track of what is going on with the loss is one of the diagnostic tools you should always make use of to verify that gradient descent is converging.\
Use again the 
\f2 \expnd0\expndtw0\kerning0
dot
\f0 \expnd0\expndtw0\kerning0
 method to compute the prediction based on vector of weights and feature part of the observation.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[126]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# function should operate on a single observation
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# input: weights, a numpy array, and lp, a LabeledPoint
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: a tuple consisting of a predicted probability and a label
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
probLabTuple\cf0 \expnd0\expndtw0\kerning0
(weights, lp):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    res = weights.dot(lp.features)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    sig = sigmoid(res)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 (sig, lp.label)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# lets check it on an easy example
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
weights_ex = np.array([\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
3\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
4\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
5\cf0 \expnd0\expndtw0\kerning0
])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
data_ex = sc.parallelize([LabeledPoint(\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, np.array([
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
])),\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
                          LabeledPoint(\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
, np.array([
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
, 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
, 
\f4\b \cf10 \expnd0\expndtw0\kerning0
-
\f2\b0 \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
]))])\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
probLab_ex = data_ex.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: probLabTuple(weights_ex, lp))\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 probLab_ex.collect()\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# you should see a list of tuples: [(0.9525741268224334, 1.0), (0.11920292202211755, 0.0)]
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# what log loss this produces? you should see the following result: 0.0877576813084
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLoss(probLab_ex)\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
[(0.9525741268224334, 1.0), (0.11920292202211755, 0.0)]\
0.0877576813084\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
Now you are ready to define a gradient descent function. There is also a small sample on which you can test it out.\
Use the subset of the data also to determine the best learning rate 
\f2 \expnd0\expndtw0\kerning0
alpha
\f0 \expnd0\expndtw0\kerning0
. Try out several values, too big learning rate will lead to increase of loss, for big ones it will appear flat or very erratic, while for appropriate rate the loss will generally decrease and flatten out after enough iterations. Too small rates will work fine but will require more iterations.\
How many iterations should you leave it run? That's difficult to say, it depends on application, you could monitor cost and after it flattened out you can stop. Better indicator is to actually compute the validation error.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[119]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# function that performs the gradient descent on a given RDD 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# inputs: dataRDD, rdd with LabeledPoint observations, 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#         noIter, integer defining number of iterations
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#         alpha, float, learning rate
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#         diagnostics, bool, should the error be recorded?
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# output: a tuple consisting of two numpy arrays, one will be final weights, 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#         the other loss in each iteration
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
def
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf11 \expnd0\expndtw0\kerning0
gradientDescent\cf0 \expnd0\expndtw0\kerning0
(dataRDD, noIter, alpha, diagnostics):\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# some help variables and arrays for storing the results
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    noObs = dataRDD.count()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    noFeatures = \cf8 \expnd0\expndtw0\kerning0
len\cf0 \expnd0\expndtw0\kerning0
(dataRDD.take(\cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
)[\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
].features)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    error = np.zeros(noIter)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# we will keep track of misclassification error with each iteration
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    weights = np.zeros(noFeatures)  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# initialize weights to zero's
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f3\i \cf5 \expnd0\expndtw0\kerning0
# we iterate until convergence, or in our case, defined by fixed number of iterations
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 i 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf8 \expnd0\expndtw0\kerning0
range\cf0 \expnd0\expndtw0\kerning0
(noIter):\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# we will also keep track of the loss, compute the loss here with the current weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# use probLabTuple() function defined above and apply it on whole dataRDD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f4\b \cf8 \expnd0\expndtw0\kerning0
if
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 diagnostics:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
            probLab = dataRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: probLabTuple(weights, x)) 
\f3\i \cf5 \expnd0\expndtw0\kerning0
###
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
            error[i] = logLoss(probLab) 
\f3\i \cf5 \expnd0\expndtw0\kerning0
###
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# compute mean gradient (computed in the driver node)
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# use the gradient() function you defined earlier
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# meanGradient should be a DenseVector and have same number of features as weights vector
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        meanGradient = dataRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: gradient(weights,x)).mean()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        weights += alpha 
\f4\b \cf10 \expnd0\expndtw0\kerning0
*
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
meanGradient  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# we update the weights based on the totalGradient
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
if
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 diagnostics:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 weights, error\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
else
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f4\b \cf8 \expnd0\expndtw0\kerning0
return
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 weights\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# ----
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# Check it on a smaller subset of data
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# ----
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# subset of the data
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
data_ex = (sc\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
            .parallelize(trainRDD.take(\cf6 \expnd0\expndtw0\kerning0
300\cf0 \expnd0\expndtw0\kerning0
))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
            .\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: LabeledPoint(lp.label, lp.features)))\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 data_ex.take(\cf6 \expnd0\expndtw0\kerning0
2\cf0 \expnd0\expndtw0\kerning0
)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# set the parameters, you can use a smaller subset to verify the learning rate
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# try other alpha values 76, 36, 26, 6, 0.6
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
alpha = \cf6 \expnd0\expndtw0\kerning0
.6\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
noIter = \cf6 \expnd0\expndtw0\kerning0
100\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
diagnostics = \cf8 \expnd0\expndtw0\kerning0
True\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# run the GD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
weights_ex, error_ex = gradientDescent(data_ex, noIter, alpha, diagnostics)\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
"weights\\n"\cf0 \expnd0\expndtw0\kerning0
, weights_ex\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
"Log loss\\n"\cf0 \expnd0\expndtw0\kerning0
, error_ex\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
[LabeledPoint(0.0, [0.481481481481,0.0530703538024,0.957379279075,0.859735349716,0.0,0.9375,0.698752922837,0.602510460251]), LabeledPoint(0.0, [0.493827160494,0.0302968686458,0.957379279075,0.859735349716,0.0,0.9375,0.698752922837,0.602510460251])]\
weights\
[-0.39456649  0.1418005  -1.01945891 -0.91548343  0.         -0.9982906\
 -0.74406237 -0.64157923]\
Log loss\
[ 0.69314718  0.33640278  0.23385035  0.18971767  0.165975    0.15148271\
  0.14189682  0.13519459  0.13031451  0.12664953  0.12382921  0.12161575\
  0.11985017  0.11842256  0.11725482  0.11629007  0.11548611  0.114811\
  0.11424027  0.11375484  0.11333973  0.11298299  0.11267503  0.11240807\
  0.11217577  0.1119729   0.11179514  0.11163888  0.1115011   0.11137928\
  0.11127125  0.1111752   0.11108957  0.11101303  0.11094444  0.11088281\
  0.1108273   0.11077717  0.11073178  0.11069057  0.11065307  0.11061883\
  0.1105875   0.11055874  0.11053227  0.11050783  0.11048521  0.1104642\
  0.11044463  0.11042635  0.11040921  0.11039311  0.11037792  0.11036356\
  0.11034994  0.11033698  0.11032461  0.11031278  0.11030142  0.1102905\
  0.11027996  0.11026977  0.1102599   0.1102503   0.11024096  0.11023185\
  0.11022295  0.11021423  0.11020568  0.11019728  0.11018901  0.11018087\
  0.11017284  0.11016491  0.11015707  0.11014931  0.11014163  0.11013402\
  0.11012646  0.11011896  0.11011151  0.1101041   0.11009674  0.11008941\
  0.11008211  0.11007485  0.11006761  0.1100604   0.11005321  0.11004604\
  0.11003889  0.11003175  0.11002463  0.11001753  0.11001043  0.11000335\
  0.10999628  0.10998922  0.10998217  0.10997512]\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
Now lets check how the loss is behaving in each iteration. If loss is not decreasing over time, you should try other learning rates, and if you don't get any improvements, something might be wrong with your gradient function.\
Note that we computed the loss over whole dataset. This works on a smaller subset when you are trying to set the learning rate, with the whole dataset (if it is a big one) this might take too much time and you should turn off that computation. If you use stochastic gradient descent instead, you can compute loss of the same observation or batch of observations that are used in the stochastic gradient descent (this is a small extension of the current gradientDescent function where the weight vector is updated after processing each observation or a small batch of randomly chosen observations).\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[120]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf10 \cb4 \expnd0\expndtw0\kerning0
%
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
matplotlib inline\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 matplotlib\cb1 \expnd0\expndtw0\kerning0
\

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 matplotlib.pyplot 
\f4\b \cf8 \expnd0\expndtw0\kerning0
as
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 plt\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# do some diagnostics
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
x = \cf8 \expnd0\expndtw0\kerning0
range\cf0 \expnd0\expndtw0\kerning0
(\cf6 \expnd0\expndtw0\kerning0
0\cf0 \expnd0\expndtw0\kerning0
, \cf8 \expnd0\expndtw0\kerning0
len\cf0 \expnd0\expndtw0\kerning0
(error_ex))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
plt.plot(x, error_ex)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
plt.title(\cf7 \expnd0\expndtw0\kerning0
'Log loss change with iterations'\cf0 \expnd0\expndtw0\kerning0
)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
plt.show()\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
{{\NeXTGraphic unknown.png \width7560 \height5320 \noorient
}}\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs36 \cf0 \expnd0\expndtw0\kerning0
Apply gradient descent on whole dataset\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
Now let's train the logistic regression model on all of our training data and evaluate its accuracy on the validation set. You would most likely need many more iterations than 500 to get better estimates.\
With the dataset of this size gradient descent works fine and you can easily run 10000 iterations on this dataset. However, on big datasets you would use stochastic version. We will not implement it, instead, now that you understand much better map-reduce operations that are going on under the hood, go to the next section and use MLlib function for training the logistic regression. That function uses stochastic gradient descent and provides you with several other useful options.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[123]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# working
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
num = trainRDD.count()\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
num\cb1 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720
\cf9 \expnd0\expndtw0\kerning0
Out[123]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
28850\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[129]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# set the parameters, setting diagnostics to True will increase the time 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# with 14 workers this took about 5 min
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
alpha = \cf6 \expnd0\expndtw0\kerning0
6\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
noIter = \cf6 \expnd0\expndtw0\kerning0
500\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
diagnostics = \cf8 \expnd0\expndtw0\kerning0
False\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# run the gradient descent on whole training set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#num = trainRDD.count()
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#data_gd = (sc
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
           
\f3\i \cf5 \expnd0\expndtw0\kerning0
#.parallelize(trainRDD.take(num))
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
          
\f3\i \cf5 \expnd0\expndtw0\kerning0
#.map(lambda x: LabeledPoint(x.label, x.features)))
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
#weights_gd = gradientDescent(data_gd, noIter, alpha, diagnostics)
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# run it on a small set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
data_gd = (sc\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
           .parallelize(trainRDD.take(\cf6 \expnd0\expndtw0\kerning0
10\cf0 \expnd0\expndtw0\kerning0
))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
          .\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: LabeledPoint(x.label, x.features)))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
weights_gd = gradientDescent(data_gd, noIter, alpha, diagnostics)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
"weights\\n"\cf0 \expnd0\expndtw0\kerning0
, weights_gd\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
weights\
[-1.03773983 -0.11914846 -2.94191958 -2.64187069  0.         -2.88083278\
 -2.14718968 -1.85144734]\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[131]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# threshold parameter for misclassification
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
threshold = \cf6 \expnd0\expndtw0\kerning0
0.5\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# compute the log loss and missclassification error on training 
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabTrain = trainRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: probLabTuple(weights_gd, x)) \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossTrain_gd = logLoss(probLabTrain)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorTrain_gd = misError(probLabTrain, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossTrain_gd, misErrorTrain_gd\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# and validation set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabVal = valRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: probLabTuple(weights_gd, x)) 
\f3\i \cf5 \expnd0\expndtw0\kerning0
## weights from training set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossVal_gd = logLoss(probLabVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorVal_gd = misError(probLabVal, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossVal_gd, misErrorVal_gd\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.653115639989 1.0\
0.604573803746 1.0\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[132]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# compare it with the logLoss of the baseline model
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# training set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabTrain = trainRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: probLabTuple(weights_gd, x))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossTrain_base = logLoss(probLabTrain)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossTrain_base, baseTrain_misError\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# validation set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabVal = valRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: probLabTuple(weights_gd, x))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossVal_base = logLoss(probLabVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossVal_base, baseVal_misError\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.653115639989 0.112824956672\
0.604573803746 0.106210475109\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs44 \cf0 \expnd0\expndtw0\kerning0
6. Logistic regression with MLlib\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
We will now use {\field{\*\fldinst{HYPERLINK "https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 LogisticRegressionWithSGD}} function from the MLlib Spark library. It returns a {\field{\*\fldinst{HYPERLINK "https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LogisticRegressionModel"}}{\fldrslt \cf2 \expnd0\expndtw0\kerning0
\ul \ulc2 LogisticRegressionModel}} with plenty of useful methods. You can access the weight by using the 
\f2 \expnd0\expndtw0\kerning0
LogisticRegressionModel.weights
\f0 \expnd0\expndtw0\kerning0
 attributes.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[133]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
from
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 pyspark.mllib.classification 
\f4\b \cf8 \expnd0\expndtw0\kerning0
import
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 LogisticRegressionWithSGD\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# set parameters
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
noIter = \cf6 \expnd0\expndtw0\kerning0
500\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
alpha = \cf6 \expnd0\expndtw0\kerning0
60\cf0 \expnd0\expndtw0\kerning0
  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# MLlib algorithm uses decaying learning rate, so we need to increase it
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
batchSize = \cf6 \expnd0\expndtw0\kerning0
0.003\cf0 \expnd0\expndtw0\kerning0
  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# defined as proportion of the whole dataset, ~100 observations
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
regLambda = \cf6 \expnd0\expndtw0\kerning0
1e-6\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
regType = \cf7 \expnd0\expndtw0\kerning0
'l2'\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
intercept = \cf8 \expnd0\expndtw0\kerning0
False\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[134]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# training the model on trainRDD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
model_sgd = LogisticRegressionWithSGD.train(data=trainRDD, iterations=noIter, step=alpha, regParam=regLambda, \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
                                            miniBatchFraction=batchSize,  regType=regType, intercept=intercept)\cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
# training the model on trainRDD\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
model_sgd = LogisticRegressionWithSGD.train(data = trainRDD, iterations=noIter, step = alpha, regParam = regLambda, miniBatchFraction = batchSize,  regType = regType, intercept = intercept)\cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[135]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# check the weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
weights_sgd = model_sgd.weights\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 weights_sgd\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
[-0.193170173347,11.1841622036,1.09573508849,-3.83195363143,0.805974510434,-1.60212939973,0.324102280463,0.270833900666]\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[136]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# evaluate the model
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# create probLabRDD and compute the log loss and missclassification error on training, using the new SGD weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabTrain = trainRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: probLabTuple(weights_sgd, lp))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossTrain_sgd = logLoss(probLabTrain)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorTrain_sgd = misError(probLabTrain, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossTrain_sgd, misErrorTrain_sgd\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# for validation set, using the new SGD weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabVal = valRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: probLabTuple(weights_sgd, lp))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossVal_sgd = logLoss(probLabVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorVal_sgd = misError(probLabVal, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossVal_sgd, misErrorVal_sgd\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
0.24353995891 1.0\
0.232023937102 1.0\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0\b\fs36 \cf0 \expnd0\expndtw0\kerning0
Optimizing the hyperparameters\cf2 \expnd0\expndtw0\kerning0
\'b6\cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\b0\fs28 \cf0 \expnd0\expndtw0\kerning0
Do a grid search to find a good regularization parameter lambda and good learning rate.\
Try 
\f2 \expnd0\expndtw0\kerning0
regLambda
\f0 \expnd0\expndtw0\kerning0
 values [1e-6, 1e-3, 1] and alpha values [0.1, 1, 10, 50, 100].\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[138]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
noIter = \cf6 \expnd0\expndtw0\kerning0
500\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
batchSize = \cf6 \expnd0\expndtw0\kerning0
0.003\cf0 \expnd0\expndtw0\kerning0
  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# defined as proportion of the whole dataset, ~100 observations
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
regType = \cf7 \expnd0\expndtw0\kerning0
'l2'\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
intercept = \cf8 \expnd0\expndtw0\kerning0
False\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
modelLoss = []  
\f3\i \cf5 \expnd0\expndtw0\kerning0
# storing losses of each model
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 alpha 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 [\cf6 \expnd0\expndtw0\kerning0
0.1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
10\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
50\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
100\cf0 \expnd0\expndtw0\kerning0
]:\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
    
\f4\b \cf8 \expnd0\expndtw0\kerning0
for
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 regLambda 
\f4\b \cf8 \expnd0\expndtw0\kerning0
in
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 [\cf6 \expnd0\expndtw0\kerning0
1e-6\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1e-3\cf0 \expnd0\expndtw0\kerning0
, \cf6 \expnd0\expndtw0\kerning0
1\cf0 \expnd0\expndtw0\kerning0
]:\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        model = ( LogisticRegressionWithSGD.train(data=trainRDD, iterations=noIter, step=alpha, regParam=regLambda, \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
                                            miniBatchFraction=batchSize,  regType=regType, intercept=intercept) )\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# evaluate the model
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        probLabVal = valRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 x: probLabTuple(weights_sgd,x))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        logLossVal = logLoss(probLabVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        modelLoss.append(logLossVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f3\i \cf5 \expnd0\expndtw0\kerning0
# some printout in each iteration
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
        
\f4\b \cf8 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 \cf7 \expnd0\expndtw0\kerning0
'alpha = \{0:.0e\}, lambda = \{1\}, loss = \{2:.3f\}'\cf0 \expnd0\expndtw0\kerning0
.\cf8 \expnd0\expndtw0\kerning0
format\cf0 \expnd0\expndtw0\kerning0
(alpha, regLambda, logLossVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\
\
\
alpha = 1e-01, lambda = 1e-06, loss = 0.232\
alpha = 1e-01, lambda = 0.001, loss = 0.232\
alpha = 1e-01, lambda = 1, loss = 0.232\
alpha = 1e+00, lambda = 1e-06, loss = 0.232\
alpha = 1e+00, lambda = 0.001, loss = 0.232\
alpha = 1e+00, lambda = 1, loss = 0.232\
alpha = 1e+01, lambda = 1e-06, loss = 0.232\
alpha = 1e+01, lambda = 0.001, loss = 0.232\
alpha = 1e+01, lambda = 1, loss = 0.232\
alpha = 5e+01, lambda = 1e-06, loss = 0.232\
alpha = 5e+01, lambda = 0.001, loss = 0.232\
alpha = 5e+01, lambda = 1, loss = 0.232\
alpha = 1e+02, lambda = 1e-06, loss = 0.232\
alpha = 1e+02, lambda = 0.001, loss = 0.232\
alpha = 1e+02, lambda = 1, loss = 0.232\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f0 \cf0 \expnd0\expndtw0\kerning0
Lambda of 1 and alpha of 50 seems to be a good solution. Lets use it for final evaluation.\
\pard\pardeftab720

\f2 \cf3 \expnd0\expndtw0\kerning0
In\'a0[140]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# set parameters
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
noIter = \cf6 \expnd0\expndtw0\kerning0
500\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
alpha = \cf6 \expnd0\expndtw0\kerning0
50\cf0 \expnd0\expndtw0\kerning0
  \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
batchSize = \cf6 \expnd0\expndtw0\kerning0
0.003\cf0 \expnd0\expndtw0\kerning0
  \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
regLambda = \cf6 \expnd0\expndtw0\kerning0
1\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
regType = \cf7 \expnd0\expndtw0\kerning0
'l2'\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
intercept = \cf8 \expnd0\expndtw0\kerning0
False\cf0 \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# train it on trainRDD
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
theModel = (LogisticRegressionWithSGD.train(data=trainRDD, iterations=noIter, step=alpha, regParam=regLambda, \cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
                                            miniBatchFraction=batchSize,  regType=regType, intercept=intercept) )\cb1 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[141]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# evaluate the final model
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# create probLabRDD and compute the log loss and missclassification error on training, using the new SGD weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabTrain = trainRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: probLabTuple(theModel.weights, lp))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossTrain_sgd = logLoss(probLabTrain)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorTrain_sgd = misError(probLabTrain, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossTrain_sgd, misErrorTrain_sgd\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# for validation set, using the new SGD weights
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabVal = valRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: probLabTuple(theModel.weights, lp))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossVal_sgd = logLoss(probLabVal)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorVal_sgd = misError(probLabVal, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossVal_sgd, misErrorVal_sgd\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
22.4707562527 0.887175043328\
22.6382907252 0.893789524891\
\pard\pardeftab720
\cf3 \expnd0\expndtw0\kerning0
In\'a0[142]:\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
\
\
\
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
\'a0\cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f3\i \cf5 \cb4 \expnd0\expndtw0\kerning0
# finally, assesing our final chosen model on the test set
\f2\i0 \cf0 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720
\cf0 \cb4 \expnd0\expndtw0\kerning0
probLabTest = testRDD.\cf8 \expnd0\expndtw0\kerning0
map\cf0 \expnd0\expndtw0\kerning0
(
\f4\b \cf8 \expnd0\expndtw0\kerning0
lambda
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 lp: probLabTuple(theModel.weights, lp))\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
logLossTest_sgd = logLoss(probLabTest)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
misErrorTest_sgd = misError(probLabTest, threshold)\cb1 \expnd0\expndtw0\kerning0
\
\cb4 \expnd0\expndtw0\kerning0
\uc0\u8203 \cb1 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720

\f4\b \cf8 \cb4 \expnd0\expndtw0\kerning0
print
\f2\b0 \cf0 \expnd0\expndtw0\kerning0
 logLossTest_sgd, misErrorTest_sgd\cb1 \expnd0\expndtw0\kerning0
\
\
\
\
22.3322022283 0.881704748015}