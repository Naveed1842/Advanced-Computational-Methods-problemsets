# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(y[neighbors[1:k, obs]]) #need to change if more than 2 categories (and next lines)
}
# predicted label
predictedClasses <- ifelse(prob > 0.5, 1, 0)
# examine the performance, available only if training
if (type == "train") {
errorCount <- table(predictedClasses, y)
accuracy <- mean(predictedClasses == y)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount))
}
trainResults <- kNN.HS(X = dataset[,1:2], y = dataset[,4],
k = k, p = p, type = "train")
head(trainResults$predictedClasses)
head(trainResults$prob)
trainResults$errorCount
trainResults$accuracy
testResults <- kNN.HS(X = datasetTest[,1:2], y = dataset[,4],
memory = dataset[,1:2],
k = k, p = p, type = "predict")
head(testResults$predictedClasses)
head(testResults$prob)
mean(testResults$predictedClasses == datasetTest[,4])
testResults <- kNN(features = datasetTest[,1:2], labels = dataset[,4],
memory = dataset[,1:2],
k = k, p = p, type = "predict")
head(testResults$predictedClasses)
features = datasetTest[,1:2]
labels = dataset[,4]
memory = dataset[,1:2]
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
type = "predict"
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
noObs <- nrow(features)
noVars <- ncol(features)
testResults <- kNN(features = datasetTest[,1:2], labels = dataset[,4],
+                    memory = dataset[,1:2],
+                    k = k, p = p, type = "predict", distanceMatrix=NULL)
distanceMatrix=NULL
is.null(distanceMatrix)
(type == "predict")
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
obs=1
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
head(probeExpanded)
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
head(distMatrix)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
head(distMatrix)
neighbors <- apply(distMatrix, 1, order)
head(neighbors)
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
i = 1
obs=1
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
classCounts
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
errorCount <- NA
accuracy <- NA
head(testResults$predLabels)
head(testResults$prob)
mean(testResults$predLabels == datasetTest[,4])
train <- read.csv("MNIST_training.csv", header=FALSE)
test <- read.csv("MNIST_test.csv", header=FALSE)
sampleRows <- train[sample(nrow(train), 1000), ]
train.train <- sampleRows[1:500, ]
train.validation <- sampleRows[501:1000, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
aaa <- kNN(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL)
system.time(kNN(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
bbb <- kNN.base(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL)
system.time(bbb)
system.time(kNN.base(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
set.seed(1234)
sampleRows <- train[sample(nrow(train), 2000), ]
train.train <- sampleRows[1:1000, ]
train.validation <- sampleRows[1001:2000, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
system.time(kNN(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
system.time(kNN.base(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
kNN <- function(features, labels, memory=NULL, k = 1, p = 2, type = 'train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the training X
### Only calculate half of matrix. Copy the other half (since symmetric for training data)
if (p %in% c(1,2)) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, ] <- apply(abs(features[(obs+1):noObs,] - probeExpanded[(obs+1):noObs,]), 1, max)
}
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
genXOR <- function(noObs=50, seed=1111, saveData=TRUE, savePlot=TRUE) {
# load the required libraries
library(assertthat)
library(mvtnorm)
library(ggplot2)
# check the inputs
assert_that(is.scalar(noObs) && is.double(noObs))
assert_that(is.scalar(seed) && is.double(seed))
assert_that(is.scalar(saveData) && is.logical(saveData))
assert_that(is.scalar(savePlot) && is.logical(savePlot))
# defining a function for generating bivariate normal data
genBVN <- function(n = 1, muXY = c(0,1), sigmaXY = diag(2)) {
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# generate XOR data and add some simple names
set.seed(seed)
class1 <- rbind(genBVN(noObs, c(1,1), diag(2)),
genBVN(noObs, c(10,10), diag(2)) )
class2 <- rbind(genBVN(noObs, c(1,10), diag(2)),
genBVN(noObs, c(10,1), diag(2)) )
dataset <- rbind(cbind(class1, 0), cbind(class2, 1))
dataset <- as.data.frame(dataset)
colnames(dataset) <- c("x1", "x2", "y")
return(dataset)
}
dataset <- genXOR(noObs=50)
results <- kNN(dataset[,1:2], dataset[,3], k=3, p=2)
output <- cbind(dataset, predLabels = results$predLabels, prob = results$prob)
write.csv(output, file="predictions.csv", row.names = FALSE)
head(output)
results$accuracy
X1 <- seq(min(dataset$x1), max(dataset$x1), by=0.2)
X2 <- seq(min(dataset$x2), max(dataset$x2), by=0.2)
grid <- expand.grid(x1=X1, x2=X2)
predGrid <- kNN(features=grid, labels=dataset[,3], memory=dataset[,1:2], k=3, p=2,
type="predict")
predGridClasses <- predGrid$predLabels
probs <- matrix(predGridClasses, length(X1), length(X2))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
theme_bw() +
theme(text = element_text(family = "Helvetica"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN")
theme_bw() +
theme(text = element_text(family = "Helvetica"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(lineheight=.8, face="bold"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(lineheight=1, face="bold"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(lineheight=2, face="bold"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(lineheight=5, face="bold"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=10, face="bold"))
# plot and save
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=20, face="bold"))
ggsave("plot.pdf", scale = 1, width = 4, height = 4)
ggsave("plot.pdf", scale = 1, width = 10, height = 10)
X1 <- seq(min(dataset$x1)-1, max(dataset$x1)+1, by=0.2)
X2 <- seq(min(dataset$x2)-1, max(dataset$x2)+1, by=0.2)
grid <- expand.grid(x1=X1, x2=X2)
predGrid <- kNN(features=grid, labels=dataset[,3], memory=dataset[,1:2], k=3, p=2,
type="predict")
predGridClasses <- predGrid$predLabels
probs <- matrix(predGridClasses, length(X1), length(X2))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=20, face="bold"))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=20, face="bold")) +
xlim(min(X1), max(X1))
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=20, face="bold")) +
xlim(min(X1)+1, max(X1)-1)
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=20, face="bold"))
ggsave("plot.pdf", scale = 1, width = 10, height = 10)
train <- read.csv("MNIST_training.csv", header=FALSE)
test <- read.csv("MNIST_test.csv", header=FALSE)
sampleRows <- train[sample(nrow(train), 6000), ]
train.train <- sampleRows[1:3000, ]
.7*6000
set.seed(1234)
sampleRows <- train[sample(nrow(train), 6000), ]
train.train <- sampleRows[1:4200, ]
train.validation <- sampleRows[4201:6000, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
k <- c( 1,3,5,7,9,11 )
p <- c( 1,2,Inf )
accuracyMatrix <- matrix(NA, length(p), length(k))
for (i in 1:length(p)) {
cat(".")
distMatrix.prev <- NULL
for (j in 1:length(k)) {
cat(".")
results <- kNN(features, labels, memory, k=k[j], p=p[i], type='predict',
distanceMatrix=distMatrix.prev)
accuracyMatrix[i,j] <- mean(results$predLabels == true.labels)
distMatrix.prev <- results$distMatrix
}
}
indices <- which(accuracyMatrix == max(accuracyMatrix), arr.ind = TRUE)
paste("Highest accuracy occurs when p=", p[indices[1,1]], " and k=",
k[indices[1,2]], sep='')
if (!require("class")) install.packages("class")
features <- as.matrix(train[,2:257])
labels <- train[,1]
k <- c( 1,3,5,7,9,11 )
accuracy <- rep(NA, length(k))
for (i in 1:length(k)) {
cat(".")
predictedClasses <- knn.cv(features, labels, k = k[i], prob = TRUE)
accuracy[i] <- mean(predictedClasses == train[,1])
}
plot(k,accuracy, type='l')
k_opt <- which.max(accuracy)
k_opt
resultsTest <- class::kNN(train[,2:257], test, train[,1])
?knn
resultsTest <- knn(train[,2:257], test, train[,1])
head(resultsTest)
length(resultsTest)
write.csv(resultsTest, file="MNIST_predictions.csv", row.names = FALSE)
aaa <- matrix(c(1,2,3,4,5,6,7,8,9), 3, 3)
aaa
aaa[lower.tri(aaa)]
bbb <- matrix(NA, 3,3)
bbb
upper.tri(bbb) <- lower.tri(aaa)
library(gdata)
install.packages(gdata)
install.packages("gdata")
library(gdata)
aaa
bbb
upperTriangle(bbb) <- t(lower.Triangle(aaa))
upperTriangle(bbb) <- t(lowertriangle(aaa))
upperTriangle(bbb) <- t(lowerTriangle(aaa))
bbb
genXOR <- function(noObs=50, seed=1111, saveData=TRUE, savePlot=TRUE) {
# load the required libraries
library(assertthat)
library(mvtnorm)
library(ggplot2)
# check the inputs
assert_that(is.scalar(noObs) && is.double(noObs))
assert_that(is.scalar(seed) && is.double(seed))
assert_that(is.scalar(saveData) && is.logical(saveData))
assert_that(is.scalar(savePlot) && is.logical(savePlot))
# defining a function for generating bivariate normal data
genBVN <- function(n = 1, muXY = c(0,1), sigmaXY = diag(2)) {
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# generate XOR data and add some simple names
set.seed(seed)
class1 <- rbind(genBVN(noObs, c(1,1), diag(2)),
genBVN(noObs, c(10,10), diag(2)) )
class2 <- rbind(genBVN(noObs, c(1,10), diag(2)),
genBVN(noObs, c(10,1), diag(2)) )
dataset <- rbind(cbind(class1, 0), cbind(class2, 1))
dataset <- as.data.frame(dataset)
colnames(dataset) <- c("x1", "x2", "y")
return(dataset)
}
dataset <- genXOR(noObs=50)
save.image("~/Desktop/BGSE/AdvancedCompMethods/Advanced-Computational-Methods-problemsets/PS4/workspace_knn.RData")
