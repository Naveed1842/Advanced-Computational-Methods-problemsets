distanceMatrix=distMatrix.prev)
accuracyMatrix[i,j] <- mean(results$predLabels == true.labels)
distMatrix.prev <- results$distMatrix
}
}
set.seed(1234)
sampleRows <- sample(nrow(train), 500)
train.train <- train[sampleRows, ]
train.validation <- train[-sampleRows, ][sampleRows, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
class(memory)
dim(memory)
class(labels)
length(labels)
class(features)
dim(features)
class(true.labels)
length(true.labels)
k <- c( 1,3,5 )
p <- c(1,2, Inf)
accuracyMatrix <- matrix(NA, length(p), length(k))
for (i in 1:length(p)) {
cat(".")
distMatrix.prev <- NULL
for (j in 1:length(k)) {
cat(".")
results <- kNN(features, labels, memory, k=k[j], p=p[i], type='predict',
distanceMatrix=distMatrix.prev)
accuracyMatrix[i,j] <- mean(results$predLabels == true.labels)
distMatrix.prev <- results$distMatrix
}
}
accuracyMatrix
mean(results$predLabels == true.labels
)
summary(results$predLabels)
summary(true.labels)
set.seed(1234)
sampleRows <- sample(nrow(train), 500)
train.train <- train[sampleRows, ]
head(train.train)
train.validation <- train[-sampleRows, ][sampleRows, ]
head(train.validation)
sampleRows <- sample(nrow(train), 1000)
sampleRows <- train[sample(nrow(train), 1000), ]
set.seed(1234)
sampleRows <- train[sample(nrow(train), 1000), ]
train.train <- sampleRows[1:500, ]
train.validation <- sampleRows[501:1000, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
summary(true.labels)
k <- c( 1,3,5 )
p <- c(1,2, Inf)
accuracyMatrix <- matrix(NA, length(p), length(k))
for (i in 1:length(p)) {
cat(".")
distMatrix.prev <- NULL
for (j in 1:length(k)) {
cat(".")
results <- kNN(features, labels, memory, k=k[j], p=p[i], type='predict',
distanceMatrix=distMatrix.prev)
accuracyMatrix[i,j] <- mean(results$predLabels == true.labels)
distMatrix.prev <- results$distMatrix
}
}
accuracyMatrix
?which.max
which(accuracyMatrix == max(accuracyMatrix), arr.ind = TRUE)
?paste
indices <- which(accuracyMatrix == max(accuracyMatrix), arr.ind = TRUE)
class(indices)
paste("Highest accuracy occurs when p=", indicies[1,1], " and k=", indicies[1,2])
paste("Highest accuracy occurs when p=", indices[1,1], " and k=", indices[1,2])
paste("Highest accuracy occurs when p=", indices[1,1], " and k=", indices[1,2], sep='')
paste("Highest accuracy occurs when p=", p[indices[1,1]], " and k=", k[indices[1,2]], sep='')
kNN.base <- function(features, labels, memory=NULL, k=1, p=2, type='train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the
# training X
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(features -  probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
kNN <- function(features, labels, memory=NULL, k = 1, p = 2, type = 'train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the training X
### Only calculate half of matrix. Copy the other half (since symmetric for training data)
if (p %in% c(1,2)) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, ] <- apply(abs(features[(obs+1):noObs,] - probeExpanded[(obs+1):noObs,]), 1, max)
}
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
genXOR <- function(noObs=50, seed=1111, saveData=TRUE, savePlot=TRUE) {
# load the required libraries
library(assertthat)
library(mvtnorm)
library(ggplot2)
# check the inputs
assert_that(is.scalar(noObs) && is.double(noObs))
assert_that(is.scalar(seed) && is.double(seed))
assert_that(is.scalar(saveData) && is.logical(saveData))
assert_that(is.scalar(savePlot) && is.logical(savePlot))
# defining a function for generating bivariate normal data
genBVN <- function(n = 1, muXY = c(0,1), sigmaXY = diag(2)) {
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# generate XOR data and add some simple names
set.seed(seed)
class1 <- rbind(genBVN(noObs, c(1,1), diag(2)),
genBVN(noObs, c(10,10), diag(2)) )
class2 <- rbind(genBVN(noObs, c(1,10), diag(2)),
genBVN(noObs, c(10,1), diag(2)) )
dataset <- rbind(cbind(class1, 0), cbind(class2, 1))
dataset <- as.data.frame(dataset)
colnames(dataset) <- c("x1", "x2", "y")
return(dataset)
}
dataset <- genXOR(noObs=50)
system.time(kNN(dataset[,1:2], dataset[,3], k=3, p=2))
system.time(kNN.base(dataset[,1:2], dataset[,3], k=3, p=2))
aaa <- kNN(dataset[,1:2], dataset[,3], k=3, p=2)
aaa$accuracy
aaa <- kNN(dataset[,1:2], dataset[,3], k=51, p=2)
aaa$accuracy
if (!require("mvtnorm")) install.packages("mvtnorm")
genGaussMix <- function(noObs = c(100, 100),
noGaussians = 10,
mixtureProb = rep(1/noGaussians, noGaussians),
seed = 2222) {
set.seed(seed)
# producing means of our bivariate Gaussians
meansC1 <- rmvnorm(noGaussians, mean = c(1,0), sigma = diag(2))
meansC2 <- rmvnorm(noGaussians, mean = c(0,1), sigma = diag(2))
# for each observation we first randomly select one Gaussian and then
# generate a point according to the parameters of that Gaussian
whichGaussianC1 <- sample(nrow(meansC1), noObs[1],
mixtureProb, replace = TRUE)
whichGaussianC2 <- sample(nrow(meansC2), noObs[2],
mixtureProb, replace = TRUE)
# now drawing samples from selected bivariate Gaussians
drawsC1 <- whichGaussianC1 %>%
sapply(function(x) rmvnorm(1, mean = meansC1[x,],
sigma = diag(2)/5)) %>% t()
drawsC2 <- whichGaussianC2 %>%
sapply(function(x) rmvnorm(1, mean = meansC2[x,],
sigma = diag(2)/5)) %>% t()
# combining and labeling
dataset <- data.frame(rbind(drawsC1, drawsC2),
label = c(rep("C1", noObs[1]), rep("C2", noObs[2])),
y = c(rep(0, noObs[1]), rep(1, noObs[2])),
stringsAsFactors = FALSE)
return(dataset)
}
datasetTest <- genGaussMix(noObs=c(200, 200), seed=1234)
k <- 1  # odd number
p <- 2  # Manhattan (1), Euclidean (2) or Chebyshev (Inf)
dataset <- genGaussMix()
kNN.HS <- function(X, y, memory = NULL,
k = 1, p = 2, type="train") {
# test the inputs
library(assertthat)
not_empty(X); not_empty(y);
if (type == "train") {
assert_that(nrow(X) == length(y))
}
is.string(type); assert_that(type %in% c("train", "predict"))
is.count(k);
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(X) &
nrow(memory) == length(y))
}
# Compute the distance between each point and all others
noObs <- nrow(X)
# if we are making predictions on the test set based on the memory,
# we compute distances between each test observation and observations
# in our memory
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the
# training X
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(X -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(X - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory) # which =length(y) which =nrow(X) which is noObs
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(X[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = 2,
byrow = TRUE)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(y[neighbors[1:k, obs]]) #need to change if more than 2 categories (and next lines)
}
# predicted label
predictedClasses <- ifelse(prob > 0.5, 1, 0)
# examine the performance, available only if training
if (type == "train") {
errorCount <- table(predictedClasses, y)
accuracy <- mean(predictedClasses == y)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount))
}
trainResults <- kNN.HS(X = dataset[,1:2], y = dataset[,4],
k = k, p = p, type = "train")
head(trainResults$predictedClasses)
head(trainResults$prob)
trainResults$errorCount
trainResults$accuracy
testResults <- kNN.HS(X = datasetTest[,1:2], y = dataset[,4],
memory = dataset[,1:2],
k = k, p = p, type = "predict")
head(testResults$predictedClasses)
head(testResults$prob)
mean(testResults$predictedClasses == datasetTest[,4])
testResults <- kNN(features = datasetTest[,1:2], labels = dataset[,4],
memory = dataset[,1:2],
k = k, p = p, type = "predict")
head(testResults$predictedClasses)
features = datasetTest[,1:2]
labels = dataset[,4]
memory = dataset[,1:2]
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
type = "predict"
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
noObs <- nrow(features)
noVars <- ncol(features)
testResults <- kNN(features = datasetTest[,1:2], labels = dataset[,4],
+                    memory = dataset[,1:2],
+                    k = k, p = p, type = "predict", distanceMatrix=NULL)
distanceMatrix=NULL
is.null(distanceMatrix)
(type == "predict")
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
obs=1
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
head(probeExpanded)
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
head(distMatrix)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
head(distMatrix)
neighbors <- apply(distMatrix, 1, order)
head(neighbors)
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
i = 1
obs=1
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
classCounts
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
errorCount <- NA
accuracy <- NA
head(testResults$predLabels)
head(testResults$prob)
mean(testResults$predLabels == datasetTest[,4])
train <- read.csv("MNIST_training.csv", header=FALSE)
test <- read.csv("MNIST_test.csv", header=FALSE)
sampleRows <- train[sample(nrow(train), 1000), ]
train.train <- sampleRows[1:500, ]
train.validation <- sampleRows[501:1000, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
aaa <- kNN(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL)
system.time(kNN(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
bbb <- kNN.base(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL)
system.time(bbb)
system.time(kNN.base(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
set.seed(1234)
sampleRows <- train[sample(nrow(train), 2000), ]
train.train <- sampleRows[1:1000, ]
train.validation <- sampleRows[1001:2000, ]
memory <- train.train[,2:257]
labels <- train.train[,1]
features <- train.validation[,2:257]
true.labels <- train.validation[,1]
system.time(kNN(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
system.time(kNN.base(features=memory, labels=labels, memory=NULL, k=1, p=2, type='train', distanceMatrix=NULL))
