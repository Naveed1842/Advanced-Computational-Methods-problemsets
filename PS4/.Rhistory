if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
kNN <- function(features, labels, memory=NULL, k = 1, p = 2, type = 'train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
if (!(is.data.frame(features)) { features <- as.data.frame(features) }
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the training X
### Only calculate half of matrix. Copy the other half (since symmetric for training data)
if (p %in% c(1,2)) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
kNN <- function(features, labels, memory=NULL, k = 1, p = 2, type = 'train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
if (!(is.data.frame(features)))     features <- as.data.frame(features)
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the training X
### Only calculate half of matrix. Copy the other half (since symmetric for training data)
if (p %in% c(1,2)) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
genXOR <- function(noObs=50, seed=1111, saveData=TRUE, savePlot=TRUE) {
# load the required libraries
library(assertthat)
library(mvtnorm)
library(ggplot2)
# check the inputs
assert_that(is.scalar(noObs) && is.double(noObs))
assert_that(is.scalar(seed) && is.double(seed))
assert_that(is.scalar(saveData) && is.logical(saveData))
assert_that(is.scalar(savePlot) && is.logical(savePlot))
# defining a function for generating bivariate normal data
genBVN <- function(n = 1, muXY = c(0,1), sigmaXY = diag(2)) {
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# generate XOR data and add some simple names
set.seed(seed)
class1 <- rbind(genBVN(noObs, c(1,1), diag(2)),
genBVN(noObs, c(10,10), diag(2)) )
class2 <- rbind(genBVN(noObs, c(1,10), diag(2)),
genBVN(noObs, c(10,1), diag(2)) )
dataset <- rbind(cbind(class1, 0), cbind(class2, 1))
dataset <- as.data.frame(dataset)
colnames(dataset) <- c("x1", "x2", "y")
return(dataset)
}
dataset <- genXOR(noObs=50)
class(dataset[,1:2])
results <- kNN(as.matrix(dataset[,1:2]), dataset[,3], k=3, p=2)
kNN <- function(features, labels, memory=NULL, k = 1, p = 2, type = 'train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
#if (!(is.data.frame(features)))     features <- as.data.frame(features)
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the training X
### Only calculate half of matrix. Copy the other half (since symmetric for training data)
if (p %in% c(1,2)) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
results <- kNN(as.matrix(dataset[,1:2]), dataset[,3], k=3, p=2)
kNN <- function(features, labels, memory=NULL, k = 1, p = 2, type = 'train',
distanceMatrix=NULL) {
# test the inputs
library(plyr)
library(assertthat)
not_empty(features)
not_empty(labels)
if (type == "train") {
assert_that(nrow(features) == length(labels))
}
is.string(type)
assert_that(type %in% c("train", "predict"))
is.count(k)
assert_that(p %in% c(1, 2, Inf))
if (type == "predict") {
assert_that(not_empty(memory) &
ncol(memory) == ncol(features) &
nrow(memory) == length(labels))
}
if (!(is.data.frame(features)))     features <- as.data.frame(features)
# define a couple of variables
noObs <- nrow(features)
noVars <- ncol(features)
# Compute distances
if (is.null(distanceMatrix)) {
if (type == "train") {
distMatrix <- matrix(NA, noObs, noObs)
diag(distMatrix) <- 0
for (obs in 1:noObs) {
# getting the probe for the current observation (from features)
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noObs, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the training X
### Only calculate half of matrix. Copy the other half (since symmetric for training data)
if (p %in% c(1,2)) {
if (obs>1) {
distMatrix[obs, 1:(obs-1) ] <- distMatrix[1:(obs-1),obs]
}
if (obs<noObs) {
distMatrix[obs, (obs+1):noObs ] <- (rowSums((abs(features[(obs+1):noObs, ] -
probeExpanded[(obs+1):noObs, ]))^p) )^(1/p)
}
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(features - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(features[obs,])
probeExpanded <- matrix(probe, nrow = noMemory, ncol = noVars, byrow = TRUE)
# computing distances between the probe and exemplars in the memory (distMatrix not symmetric)
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory - probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
} else {
distMatrix <- distanceMatrix
}
# Sort the distances in increasing numerical order and pick the first
# neighbours for point 1 are in COLUMN 1
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
# - ties broken randomly
predictedClasses <- rep(NA, noObs)
prob <- rep(NA, noObs)
for (obs in 1:noObs) {
classCounts <- plyr::count(labels[neighbors[1:k, obs]])
possiblePredClasses <- classCounts[classCounts$freq == max(classCounts$freq),1]
predictedClasses[obs] <- possiblePredClasses[sample(length(possiblePredClasses),1)]
prob[obs] <- max(classCounts$freq)/k
}
# examine the performance
if (type == "train") {
errorCount <- table(predictedClasses, labels)
accuracy <- mean(predictedClasses == labels)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predLabels = predictedClasses,
prob = prob,
accuracy = accuracy,
errorCount = errorCount,
distMatrix = distMatrix))
}
genXOR <- function(noObs=50, seed=1111, saveData=TRUE, savePlot=TRUE) {
# load the required libraries
library(assertthat)
library(mvtnorm)
library(ggplot2)
# check the inputs
assert_that(is.scalar(noObs) && is.double(noObs))
assert_that(is.scalar(seed) && is.double(seed))
assert_that(is.scalar(saveData) && is.logical(saveData))
assert_that(is.scalar(savePlot) && is.logical(savePlot))
# defining a function for generating bivariate normal data
genBVN <- function(n = 1, muXY = c(0,1), sigmaXY = diag(2)) {
rdraws <- rmvnorm(n, mean = muXY, sigma = sigmaXY)
return(rdraws)
}
# generate XOR data and add some simple names
set.seed(seed)
class1 <- rbind(genBVN(noObs, c(1,1), diag(2)),
genBVN(noObs, c(10,10), diag(2)) )
class2 <- rbind(genBVN(noObs, c(1,10), diag(2)),
genBVN(noObs, c(10,1), diag(2)) )
dataset <- rbind(cbind(class1, 0), cbind(class2, 1))
dataset <- as.data.frame(dataset)
colnames(dataset) <- c("x1", "x2", "y")
return(dataset)
}
results <- kNN(dataset[,1:2], dataset[,3], k=3, p=2)
output <- cbind(dataset, predLabels = results$predLabels, prob = results$prob)
X1 <- seq(min(dataset$x1)-1, max(dataset$x1)+1, by=0.2)
X2 <- seq(min(dataset$x2)-1, max(dataset$x2)+1, by=0.2)
grid <- expand.grid(x1=X1, x2=X2)
# predict results for all points on grid
predGrid <- kNN(features=grid, labels=dataset[,3], memory=dataset[,1:2], k=3, p=2,
type="predict")
dataset <- genXOR(noObs=50)
results <- kNN(dataset[,1:2], dataset[,3], k=3, p=2)
output <- cbind(dataset, predLabels = results$predLabels, prob = results$prob)
X1 <- seq(min(dataset$x1)-1, max(dataset$x1)+1, by=0.2)
X2 <- seq(min(dataset$x2)-1, max(dataset$x2)+1, by=0.2)
grid <- expand.grid(x1=X1, x2=X2)
# predict results for all points on grid
predGrid <- kNN(features=grid, labels=dataset[,3], memory=dataset[,1:2], k=3, p=2,
type="predict")
predGridClasses <- predGrid$predLabels
ggplot( data=grid, aes(x=x1, y=x2, z=predGridClasses) ) +
stat_contour( bins=1, size=2) +
geom_tile(aes(fill=as.factor(predGridClasses)), alpha=0.5) +
geom_point(data=dataset, size=2, aes(x=x1, y=x2, z=y, colour=as.factor(dataset$y))) +
scale_fill_manual("Class", values=c("blue", "red")) +
scale_colour_manual("Training points", values=c("blue", "red")) +
ggtitle("Decision boundaries for kNN") +
theme_bw() +
theme(text = element_text(family = "Helvetica")) +
theme(plot.title= element_text(size=20, face="bold"))
load("/Users/annekespeijers/Desktop/BGSE/AdvancedCompMethods/Advanced-Computational-Methods-problemsets/PS4/workspace_knn.RData")
accuracyMatrix
setwd("~/Desktop/BGSE/AdvancedCompMethods/Advanced-Computational-Methods-problemsets/PS4")
